{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7741e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4076 entries, 0 to 4075\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instance_id  4076 non-null   object\n",
      " 1   sentence_1   4076 non-null   object\n",
      " 2   sentence_2   4076 non-null   object\n",
      " 3   gold_label   4076 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 127.5+ KB\n",
      "instance_id    0\n",
      "sentence_1     0\n",
      "sentence_2     0\n",
      "gold_label     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/clareschwarzenberg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/clareschwarzenberg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_csv('train_with_label.txt', sep='\\t+')\n",
    "df.columns = ['instance_id', 'sentence_1', 'sentence_2', 'gold_label']\n",
    "df.shape\n",
    "df.dtypes\n",
    "df.info()\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df2 = pd.read_csv('dev_with_label.txt', sep='\\t+')\n",
    "df2.columns = ['instance_id', 'sentence_1', 'sentence_2', 'gold_label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287c8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e50577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "        \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c08553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentence_1_Token'] = list(map(clean_text, df.sentence_1))\n",
    "df['Sentence_2_Token'] = list(map(clean_text, df.sentence_2))\n",
    "\n",
    "df2['Sentence_1_Token'] = list(map(clean_text, df2.sentence_1))\n",
    "df2['Sentence_2_Token'] = list(map(clean_text, df2.sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e426979",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "df['Sentence_1_Cleaned'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df.Sentence_1_Token))\n",
    "df['Sentence_2_Cleaned'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df.Sentence_2_Token))\n",
    "\n",
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "df2['Sentence_1_Cleaned'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df2.Sentence_1_Token))\n",
    "df2['Sentence_2_Cleaned'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df2.Sentence_2_Token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f1fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df['Sentence_1_Cleaned'] = list(map(lambda word:list(map(stemmer.stem, word)),df.Sentence_1_Cleaned))\n",
    "df['Sentence_2_Cleaned'] = list(map(lambda word:list(map(stemmer.stem, word)),df.Sentence_2_Cleaned))\n",
    "\n",
    "df2['Sentence_1_Cleaned'] = list(map(lambda word:list(map(stemmer.stem, word)),df2.Sentence_1_Cleaned))\n",
    "df2['Sentence_2_Cleaned'] = list(map(lambda word:list(map(stemmer.stem, word)),df2.Sentence_2_Cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95492b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentence_1_Length\"] = df[\"sentence_1\"].apply(lambda n: len(n.split()))\n",
    "df[\"Sentence_2_Length\"] = df[\"sentence_2\"].apply(lambda n: len(n.split()))\n",
    "\n",
    "df2[\"Sentence_1_Length\"] = df2[\"sentence_1\"].apply(lambda n: len(n.split()))\n",
    "df2[\"Sentence_2_Length\"] = df2[\"sentence_2\"].apply(lambda n: len(n.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400fa165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>Sentence_1_Token</th>\n",
       "      <th>Sentence_2_Token</th>\n",
       "      <th>Sentence_1_Cleaned</th>\n",
       "      <th>Sentence_2_Cleaned</th>\n",
       "      <th>Sentence_1_Length</th>\n",
       "      <th>Sentence_2_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_id_1</td>\n",
       "      <td>The woman was exposed to the SARS virus while ...</td>\n",
       "      <td>The woman was exposed to the SARS virus while ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[woman, exposed, sars, virus, hospital, health...</td>\n",
       "      <td>[woman, exposed, sars, virus, hospital, health...</td>\n",
       "      <td>[woman, expos, sar, viru, hospit, health, care...</td>\n",
       "      <td>[woman, expos, sar, viru, hospit, health, care...</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_id_2</td>\n",
       "      <td>He said the problem needs to be corrected befo...</td>\n",
       "      <td>He said the prob lem needs to be corrected bef...</td>\n",
       "      <td>1</td>\n",
       "      <td>[said, problem, needs, corrected, space, shutt...</td>\n",
       "      <td>[said, prob, lem, needs, corrected, space, shu...</td>\n",
       "      <td>[said, problem, need, correct, space, shuttl, ...</td>\n",
       "      <td>[said, prob, lem, need, correct, space, shuttl...</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_id_3</td>\n",
       "      <td>A representative for Phoenix-based U-Haul decl...</td>\n",
       "      <td>Anthony Citrano , a representative for WhenU ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[representative, phoenix, based, u, haul, decl...</td>\n",
       "      <td>[anthony, citrano, representative, whenu, decl...</td>\n",
       "      <td>[repres, phoenix, base, u, haul, declin, comme...</td>\n",
       "      <td>[anthoni, citrano, repres, whenu, declin, comm...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance_id                                         sentence_1  \\\n",
       "0  train_id_1  The woman was exposed to the SARS virus while ...   \n",
       "1  train_id_2  He said the problem needs to be corrected befo...   \n",
       "2  train_id_3  A representative for Phoenix-based U-Haul decl...   \n",
       "\n",
       "                                          sentence_2  gold_label  \\\n",
       "0  The woman was exposed to the SARS virus while ...           1   \n",
       "1  He said the prob lem needs to be corrected bef...           1   \n",
       "2  Anthony Citrano , a representative for WhenU ,...           0   \n",
       "\n",
       "                                    Sentence_1_Token  \\\n",
       "0  [woman, exposed, sars, virus, hospital, health...   \n",
       "1  [said, problem, needs, corrected, space, shutt...   \n",
       "2  [representative, phoenix, based, u, haul, decl...   \n",
       "\n",
       "                                    Sentence_2_Token  \\\n",
       "0  [woman, exposed, sars, virus, hospital, health...   \n",
       "1  [said, prob, lem, needs, corrected, space, shu...   \n",
       "2  [anthony, citrano, representative, whenu, decl...   \n",
       "\n",
       "                                  Sentence_1_Cleaned  \\\n",
       "0  [woman, expos, sar, viru, hospit, health, care...   \n",
       "1  [said, problem, need, correct, space, shuttl, ...   \n",
       "2  [repres, phoenix, base, u, haul, declin, comme...   \n",
       "\n",
       "                                  Sentence_2_Cleaned  Sentence_1_Length  \\\n",
       "0  [woman, expos, sar, viru, hospit, health, care...                 35   \n",
       "1  [said, prob, lem, need, correct, space, shuttl...                 19   \n",
       "2  [anthoni, citrano, repres, whenu, declin, comm...                 19   \n",
       "\n",
       "   Sentence_2_Length  \n",
       "0                 32  \n",
       "1                 17  \n",
       "2                 15  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd65a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = sklearn.model_selection.train_test_split(df, train_size = 0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beefc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['gold_label'].values\n",
    "y_test = test['gold_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37da260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,3], lowercase=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6788e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = bow_transform.fit_transform(train['Sentence_1_Cleaned'], train['Sentence_2_Cleaned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fb1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = bow_transform.transform(test['Sentence_1_Cleaned'])\n",
    "X_test_2 = bow_transform.transform(test['Sentence_2_Cleaned'])\n",
    "X_test_bow = X_test_1 + X_test_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18ebb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "X_train_tfidf = tfidf_transform.fit_transform(X_train_bow)\n",
    "X_test_tfidf = tfidf_transform.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c29f1ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2}\n",
      "0.7309893704006541\n"
     ]
    }
   ],
   "source": [
    "coeff = range(1, 10)\n",
    "param_grid = dict(C=coeff)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid = GridSearchCV(clf_lr, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train_bow, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2', C=grid.best_params_['C'])\n",
    "clf_lr.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = clf_lr.predict(X_test_bow)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3250f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = df2['gold_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5affc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_bow_1 = bow_transform.transform(df2['Sentence_1_Cleaned'])\n",
    "X_dev_bow_2 = bow_transform.transform(df2['Sentence_2_Cleaned'])\n",
    "X_dev_bow = X_dev_bow_1 + X_dev_bow_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc743eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "X_dev_tfidf = tfidf_transform.fit_transform(X_dev_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f202546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5836791147994468\n"
     ]
    }
   ],
   "source": [
    "y_dev_pred = clf_lr.predict(X_dev_bow)\n",
    "print(accuracy_score(y_dev, y_dev_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f63c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187244480784954\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clfSVM = svm.SVC(kernel='linear')\n",
    "clfSVM.fit(X_train_bow, y_train)\n",
    "predictionSVM = clfSVM.predict(X_test_bow)\n",
    "print(accuracy_score(y_test, predictionSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4151321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598893499308437\n"
     ]
    }
   ],
   "source": [
    "devSVM = clfSVM.predict(X_dev_bow)\n",
    "print(accuracy_score(y_dev, devSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a3f520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('test_without_label.txt', sep='\\t+')\n",
    "df3.columns = ['instance_id', 'sentence_1', 'sentence_2']\n",
    "\n",
    "df3['Sentence_1_Token'] = list(map(clean_text, df3.sentence_1))\n",
    "df3['Sentence_2_Token'] = list(map(clean_text, df3.sentence_2))\n",
    "\n",
    "df3['Sentence_1_Cleaned'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df3.Sentence_1_Token))\n",
    "df3['Sentence_2_Cleaned'] = list(map(lambda word:list(map(lemm.lemmatize, word)),df3.Sentence_2_Token))\n",
    "\n",
    "df2['Sentence_1_Cleaned'] = list(map(lambda word:list(map(stemmer.stem, word)),df2.Sentence_1_Cleaned))\n",
    "df2['Sentence_2_Cleaned'] = list(map(lambda word:list(map(stemmer.stem, word)),df2.Sentence_2_Cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6157c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_bow_1 = bow_transform.transform(df3['Sentence_1_Cleaned'])\n",
    "X_final_bow_2 = bow_transform.transform(df3['Sentence_2_Cleaned'])\n",
    "X_final_bow = X_final_bow_1 + X_final_bow_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823b2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSVM = clfSVM.predict(X_final_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb31519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('ClareSchwarzenberg_test_result.txt', 'a')\n",
    "for ind in df3.index:\n",
    "    file.write(df3['instance_id'][ind])\n",
    "    file.write(\"\\t\")\n",
    "    file.write(str(finalSVM[ind]))\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b12f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
